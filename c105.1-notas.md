# Clase 105 - Manejo de grandes volúmenes de datos

> **Centro de Investigación en Computación**
>
> *Instituto Politécnico Nacional*
>
> Departamento de Diplomados y Extensión Profesional
>

![CIC-IPN](https://www.cic.ipn.mx/images/logos/logositiocicletras.png)

**Profesor**: [Alan Badillo Salas](alan@nomadacode.com)

---

## Contenido

    1. Introducción a AUTO-GPT
      - Instalación de Docker
      - Clonación del repositorio de AUTO-GPT
      - Configuración del `Docker Compose`
      - Configuración del `Dockerfile`
      - Configuración del `requirements.txt`
      - Configuración del `.env`
      - Compilación del Docker Compose `auto-gpt`
      - Inicialización del Docker Compose `auto-gpt` 
    2. Generación de contenido asistida por AUTO-GPT
      - Generación de archivos de texto y markdown
      - Generación de archivos CSV
      - Consultas web y scraping
      - Generación de código de Python
      - Ejecución de código de Python
      - Procesamiento masivo

## Objetivos

Aprender a manipular grandes volúmenes de datos de Excel a través de la programación asistida por AUTO-GPT. Hacer que CHATGPT genere los programas necesarios para crear reportes estadísticos de grandes volúmenes de datos extraídos desde excel. Generar reportes de texto/CSV y PDF para mostrar los resultados del análisis.

## Introducción a AUTO-GPT

AutoGPT es una herramienta basada en Python capaz de hacer solicitudes a ChatGPT, interpretar los resultados y proponer una solución en modo de comandos para cumplir un objetivo.

La forma de operar es la siguiente:

1. Se establece un asistente al que se le solicita un objetivo.
2. El asistente crea un plan para cumplir con el objetivo (una lista de pasos)
3. El asistente detecta la complejidad y razona la solución
4. El asistente propone un comando para ser ejecutado, el cuál puede ser una búsqueda web, leer un archivo y extraer la información, escribir un archivo con el contenido generado, ejecutar un comando del sistema operativo, ejecutar un archivo de Python, ejecutar código de Python, etc.
5. El asistente razona la salida del sistema al ejecutar el comando y determina si continuar con el plan o resolver alguna actividad intermedia.
6. El usuario es capaz de confirmar los comandos ejecutados o mejorarlos mediante un input continuo.

Para hacer funcionar AutoGPT se recomienda usar Docker, para proteger el sistema operativo base y no exponer datos personales o dañarlo.

### Instalación de Docker

Docker es un sistema basado en contenedores. Los cuáles son cajas configuradas para simular un sistema operativo completo basado en Linux, el cuál se ejecutará sobre nuestro sistema operativo base (Windows, Linux o Mac).

Los contenedores se basan en imágenes que contienen todos los recursos que usará el subsistema operativo solicitado (puede ser cualquier distribución de linux).

La imagen puede ser descargada o compilada, si es compilada se necesitará un archivo `Dockerfile` que describa todos los pasos para crear la imagen y un archivo `Docker Compose` para automatizar el proceso.

La instalación de Docker se logra al descargar `Docker Desktop` para Windows, Linux o Mac desde:

[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)


![Docker install](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.44.10.png>)

Una vez instalado deberemos registrarnos o iniciar sesión para comenzar a utilizar Docker.

![Docker desktop](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.47.06.png>)

### Clonación del repositorio de AUTO-GPT

Para crear la imagen de Docker que contendrá a AutoGPT debemos clonar el repositorio o descargarlo como zip y descomprimirlo.

[https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)

> Comando para clonar `git clone https://github.com/Significant-Gravitas/Auto-GPT.git`

Lo más sencillo es descargarlo como zip y descomprimirlo.

![AutoGPT clone](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.50.59.png>)

### Configuración del `Docker Compose`

Una vez clonado o descomprimido, debemos ubicar el archivo `docker-compose.yml` y podemos editarlo con alguna configuración avanzada, por defecto no le haremos nada.

![Docker Compose](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.52.39.png>)

### Configuración del `Dockerfile`

En el archivo `Dockerfile` agregaremos `gcc` en la línea `9` (`# Install browsers`) quedando como:

```bash
# Install browsers
RUN apt-get update && apt-get install -y \
    chromium-driver firefox-esr ca-certificates gcc \
    && apt-get clean && rm -rf /var/lib/apt/lists/*
```

![Dockerfile](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.55.21.png>)

### Configuración del `requirements.txt`

En el archivo `requirements.txt` agregaremos todas las librerías de Python que queramos que estén disponibles (las que se instalan con `pip install <package>`) como `pandas`, `matplotlib`, etc.

![requirements.txt](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 7.57.59.png>)

### Configuración del `.env`

Crearemos el archivo `.env` con el siguiente contenido:

```bash
# For further descriptions of these settings see docs/configuration/options.md or go to docs.agpt.co

################################################################################
### AUTO-GPT - GENERAL SETTINGS
################################################################################

## OPENAI_API_KEY - OpenAI API Key (Example: my-openai-api-key)
OPENAI_API_KEY=YOUR_API_KEY

## EXECUTE_LOCAL_COMMANDS - Allow local command execution (Default: False)
# EXECUTE_LOCAL_COMMANDS=False
EXECUTE_LOCAL_COMMANDS=True

## RESTRICT_TO_WORKSPACE - Restrict file operations to workspace ./auto_gpt_workspace (Default: True)
# RESTRICT_TO_WORKSPACE=True

## USER_AGENT - Define the user-agent used by the requests library to browse website (string)
# USER_AGENT="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36"

## AI_SETTINGS_FILE - Specifies which AI Settings file to use, relative to the Auto-GPT root directory. (defaults to ai_settings.yaml)
# AI_SETTINGS_FILE=ai_settings.yaml

## PLUGINS_CONFIG_FILE - The path to the plugins_config.yaml file, relative to the Auto-GPT root directory. (Default plugins_config.yaml)
# PLUGINS_CONFIG_FILE=plugins_config.yaml

## PROMPT_SETTINGS_FILE - Specifies which Prompt Settings file to use, relative to the Auto-GPT root directory. (defaults to prompt_settings.yaml)
# PROMPT_SETTINGS_FILE=prompt_settings.yaml

## OPENAI_API_BASE_URL - Custom url for the OpenAI API, useful for connecting to custom backends. No effect if USE_AZURE is true, leave blank to keep the default url
# the following is an example:
# OPENAI_API_BASE_URL=http://localhost:443/v1

## OPENAI_FUNCTIONS - Enables OpenAI functions: https://platform.openai.com/docs/guides/gpt/function-calling
## WARNING: this feature is only supported by OpenAI's newest models. Until these models become the default on 27 June, add a '-0613' suffix to the model of your choosing.
# OPENAI_FUNCTIONS=False

## AUTHORISE COMMAND KEY - Key to authorise commands
# AUTHORISE_COMMAND_KEY=y

## EXIT_KEY - Key to exit AUTO-GPT
# EXIT_KEY=n

## PLAIN_OUTPUT - Plain output, which disables the spinner (Default: False)
# PLAIN_OUTPUT=False

## DISABLED_COMMAND_CATEGORIES - The list of categories of commands that are disabled (Default: None)
# DISABLED_COMMAND_CATEGORIES=

################################################################################
### LLM PROVIDER
################################################################################

## TEMPERATURE - Sets temperature in OpenAI (Default: 0)
# TEMPERATURE=0

## OPENAI_ORGANIZATION - Your OpenAI Organization key (Default: None)
# OPENAI_ORGANIZATION=

## USE_AZURE - Use Azure OpenAI or not (Default: False)
# USE_AZURE=False

## AZURE_CONFIG_FILE - The path to the azure.yaml file, relative to the Auto-GPT root directory. (Default: azure.yaml)
# AZURE_CONFIG_FILE=azure.yaml


################################################################################
### LLM MODELS
################################################################################

## SMART_LLM - Smart language model (Default: gpt-4)
# SMART_LLM=gpt-4

## FAST_LLM - Fast language model (Default: gpt-3.5-turbo)
# FAST_LLM=gpt-3.5-turbo

## EMBEDDING_MODEL - Model to use for creating embeddings
# EMBEDDING_MODEL=text-embedding-ada-002

################################################################################
### SHELL EXECUTION
################################################################################

## SHELL_COMMAND_CONTROL - Whether to use "allowlist" or "denylist" to determine what shell commands can be executed (Default: denylist)
# SHELL_COMMAND_CONTROL=denylist

## ONLY if SHELL_COMMAND_CONTROL is set to denylist:
## SHELL_DENYLIST - List of shell commands that ARE NOT allowed to be executed by Auto-GPT (Default: sudo,su)
# SHELL_DENYLIST=sudo,su

## ONLY if SHELL_COMMAND_CONTROL is set to allowlist:
## SHELL_ALLOWLIST - List of shell commands that ARE allowed to be executed by Auto-GPT (Default: None)
# SHELL_ALLOWLIST=

################################################################################
### MEMORY
################################################################################

### General

## MEMORY_BACKEND - Memory backend type
# MEMORY_BACKEND=json_file

## MEMORY_INDEX - Value used in the Memory backend for scoping, naming, or indexing (Default: auto-gpt)
# MEMORY_INDEX=auto-gpt

### Redis

## REDIS_HOST - Redis host (Default: localhost, use "redis" for docker-compose)
# REDIS_HOST=localhost

## REDIS_PORT - Redis port (Default: 6379)
# REDIS_PORT=6379

## REDIS_PASSWORD - Redis password (Default: "")
# REDIS_PASSWORD=

## WIPE_REDIS_ON_START - Wipes data / index on start (Default: True)
# WIPE_REDIS_ON_START=True

################################################################################
### IMAGE GENERATION PROVIDER
################################################################################

### Common

## IMAGE_PROVIDER - Image provider (Default: dalle)
# IMAGE_PROVIDER=dalle

## IMAGE_SIZE - Image size (Default: 256)
# IMAGE_SIZE=256

### Huggingface (IMAGE_PROVIDER=huggingface)

## HUGGINGFACE_IMAGE_MODEL - Text-to-image model from Huggingface (Default: CompVis/stable-diffusion-v1-4)
# HUGGINGFACE_IMAGE_MODEL=CompVis/stable-diffusion-v1-4

## HUGGINGFACE_API_TOKEN - HuggingFace API token (Default: None)
# HUGGINGFACE_API_TOKEN=

### Stable Diffusion (IMAGE_PROVIDER=sdwebui)

## SD_WEBUI_AUTH - Stable Diffusion Web UI username:password pair (Default: None)
# SD_WEBUI_AUTH=

## SD_WEBUI_URL - Stable Diffusion Web UI API URL (Default: http://localhost:7860)
# SD_WEBUI_URL=http://localhost:7860

################################################################################
### AUDIO TO TEXT PROVIDER
################################################################################

## AUDIO_TO_TEXT_PROVIDER - Audio-to-text provider (Default: huggingface)
# AUDIO_TO_TEXT_PROVIDER=huggingface

## HUGGINGFACE_AUDIO_TO_TEXT_MODEL - The model for HuggingFace to use (Default: CompVis/stable-diffusion-v1-4)
# HUGGINGFACE_AUDIO_TO_TEXT_MODEL=CompVis/stable-diffusion-v1-4

################################################################################
### GITHUB
################################################################################

## GITHUB_API_KEY - Github API key / PAT (Default: None)
# GITHUB_API_KEY=

## GITHUB_USERNAME - Github username (Default: None)
# GITHUB_USERNAME=

################################################################################
### WEB BROWSING
################################################################################

## HEADLESS_BROWSER - Whether to run the browser in headless mode (default: True)
# HEADLESS_BROWSER=True

## USE_WEB_BROWSER - Sets the web-browser driver to use with selenium (default: chrome)
# USE_WEB_BROWSER=chrome
USE_WEB_BROWSER=chrome

## BROWSE_CHUNK_MAX_LENGTH - When browsing website, define the length of chunks to summarize (Default: 3000)
# BROWSE_CHUNK_MAX_LENGTH=3000

## BROWSE_SPACY_LANGUAGE_MODEL - spaCy language model](https://spacy.io/usage/models) to use when creating chunks. (Default: en_core_web_sm)
# BROWSE_SPACY_LANGUAGE_MODEL=en_core_web_sm

## GOOGLE_API_KEY - Google API key (Default: None)
# GOOGLE_API_KEY=

## GOOGLE_CUSTOM_SEARCH_ENGINE_ID - Google custom search engine ID (Default: None)
# GOOGLE_CUSTOM_SEARCH_ENGINE_ID=

################################################################################
### TEXT TO SPEECH PROVIDER
################################################################################

## TEXT_TO_SPEECH_PROVIDER - Which Text to Speech provider to use (Default: gtts)
# TEXT_TO_SPEECH_PROVIDER=gtts

### Only if TEXT_TO_SPEECH_PROVIDER=streamelements
## STREAMELEMENTS_VOICE - Voice to use for StreamElements (Default: Brian)
# STREAMELEMENTS_VOICE=Brian

### Only if TEXT_TO_SPEECH_PROVIDER=elevenlabs
## ELEVENLABS_API_KEY - Eleven Labs API key (Default: None)
# ELEVENLABS_API_KEY=

## ELEVENLABS_VOICE_ID - Eleven Labs voice ID (Example: None)
# ELEVENLABS_VOICE_ID=

################################################################################
### CHAT MESSAGES
################################################################################

## CHAT_MESSAGES_ENABLED - Enable chat messages (Default: False)
# CHAT_MESSAGES_ENABLED=False
```

Pudes modificar de forma avanzada las configuraciones por defecto.

![.env](<./screenshots/Captura de pantalla 2023-08-26 a la(s) 8.01.38.png>)


**NOTA:** No debes activar el navegador web `HEADLESS_BROWSER=False` ya que se detendrá el contendor inmediatamente.

### Compilación del Docker Compose `auto-gpt`

Podemos compilar la imagen del contenedor mediante el comando:

```bash
docker compose build auto-gpt
```

### Inicialización del Docker Compose `auto-gpt` 

Podemos ejecutar la imagen, para crear un contenedor con AutoGPT funcionando mediante el comando:

```bash
docker compose run auto-gpt
```

## 2. Generación de contenido asistida por AUTO-GPT



### Generación de archivos de texto y markdown



### Generación de archivos CSV



### Consultas web y scraping



### Generación de código de Python



### Ejecución de código de Python



### Procesamiento masivo

